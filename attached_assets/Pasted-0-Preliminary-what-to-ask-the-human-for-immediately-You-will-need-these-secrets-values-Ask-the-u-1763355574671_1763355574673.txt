0) Preliminary: what to ask the human for immediately

You will need these secrets/values. Ask the user to provide these (exact environment variable names suggested):

REPLIT_POSTGRES_URL — Postgres connection string for Replit Postgres (e.g. postgres://user:pass@host:port/dbname).

SUPABASE_URL — the Supabase project URL.

SUPABASE_SERVICE_ROLE_KEY — required for server-side writes to Supabase (write/delete). Use service role for loader.

SUPABASE_ANON_KEY — for UI (read-only) if needed.

(Optional) SUPABASE_DB_SCHEMA_SQL_PATH — path to the regenerated Supabase SQL file in repo (if stored).

Confirm that data/seed/ contains the CSVs and that they are committed.

Note: do not request user passwords in plain text outside the environment variable mechanism. Prefer instructing them to add secrets to Replit's secret store / environment variables UI.

1) Confirm files are present & inspect

Commands (run in project root):

# list generated CSVs and sizes
ls -lh data/seed/

# quickly inspect first few lines of each
for f in data/seed/*.csv; do echo "== $f =="; head -n 3 "$f"; done


Expected files (and expected row counts approximate):

branches.csv — 100 rows

customers.csv — ~120,000 rows

officers.csv — linked to branches

loans.csv — ~150,000 rows

repayments.csv — many rows (loan repayments)

daily_branch_performance.csv — 36,500 rows (100 branches × 365 days)

monthly_branch_summary.csv

officer_performance.csv

fraud_signals.csv

ai_customer_features.csv

If these files are missing, do not re-run generator without explicit permission. Ask why they are missing.

2) Validate schema parity (Postgres vs Supabase)

Check Postgres schema (use REPLIT_POSTGRES_URL):

# show tables (psql)
psql "$REPLIT_POSTGRES_URL" -c "\dt"  # or use a client
# show customers table columns
psql "$REPLIT_POSTGRES_URL" -c "\d+ customers"


Check Supabase schema (if you have SUPABASE_SERVICE_ROLE_KEY):

Either open Supabase SQL editor and compare schema, or run queries from a script that points to Supabase Postgres instance.

If mismatch found, recommended approach:

If Supabase has older/corrupted schema: back it up (EXPORT SQL from Supabase).

Apply the authoritative SQL schema in repo (the regenerated one). Either:

psql <supabase_db_connection> -f path/to/supabase_schema.sql (requires DB creds)

Or use Supabase CLI: supabase db push if the project is configured.

Important: If you cannot write to Supabase DB directly, proceed to load data into Replit Postgres first and then fix Supabase schema.

3) Prepare environment & dependencies

Make sure Node and TS runner are available. Then:

# Install project node deps
npm ci

# If loader uses TS and no ts-node script, install ts-node (if allowed)
npm install -D ts-node typescript @types/node

# Ensure Python deps if loader uses python helpers (but generator already ran)
pip3 install -r requirements.txt || pip3 install pandas

4) Run the loader in DRY-RUN mode first

Modify or run the loader with a --dry-run flag if supported. If not, add logging and a dry-run mode that reads CSVs and counts rows without inserting.

Suggested command (if loader uses node):

# dry run: validate parse only (no writes)
node --loader ts-node/esm scripts/load-data.ts --dry-run
# or
npx ts-node scripts/load-data.ts --dry-run


If load-data.ts has no dry-run, open file and add a boolean DRY_RUN = true at top so it skips write code paths and just parses and validates.

What to confirm in dry-run:

CSVs parse without errors.

Row counts are as expected.

All required columns are present (match schema).

No glaring data type issues (date formats, numeric ranges, missing FKs).

5) Provide environment variables and run the real loader

Once dry-run is clean, set env vars in Replit environment (or export in shell for local run):

Required env vars:

export REPLIT_POSTGRES_URL="postgres://..."
export SUPABASE_URL="https://<project>.supabase.co"
export SUPABASE_SERVICE_ROLE_KEY="service-role-..."
export SUPABASE_ANON_KEY="..."


Run loader:

# run loader (TypeScript)
npx ts-node scripts/load-data.ts
# or if compiled JS is present:
node scripts/load-data.js


If loader supports flags for target(s):

--targets=postgres,supabase to load both

--table=customers to load one table at a time

If loader fails, capture full stack trace and logs. Common fixes:

Missing env var (Supabase key) — service role needed for inserts.

Table constraints / FKs preventing insert order — load in dependency order:

branches

officers

customers

loans

repayments

performance/fraud/features

Use transactions per-file and smaller batches (e.g., 5k rows per transaction).

6) Post-load verification SQLs (run against each DB)

Run these checks both on Replit Postgres and Supabase (if loaded):

Basic counts:

SELECT 'branches' AS t, COUNT(*) FROM branches;
SELECT 'customers' AS t, COUNT(*) FROM customers;
SELECT 'loans' AS t, COUNT(*) FROM loans;
SELECT 'daily_perf' AS t, COUNT(*) FROM daily_branch_performance;


FK sanity checks:

-- customers referencing branches (if applicable)
SELECT COUNT(*) FROM customers WHERE branch_id IS NOT NULL AND branch_id NOT IN (SELECT id FROM branches);

-- loans referencing customers
SELECT COUNT(*) FROM loans WHERE customer_id NOT IN (SELECT id FROM customers);

-- repayments referencing loans
SELECT COUNT(*) FROM repayments WHERE loan_id NOT IN (SELECT id FROM loans);


Sample row checks:

-- sample customer
SELECT id, first_name, last_name, nationalId, phone FROM customers LIMIT 5;
-- sample loan
SELECT id, customer_id, amount, status, created_at FROM loans ORDER BY created_at DESC LIMIT 5;


Expectations:

Branches = 100

Customers ≈ 120000

Loans ≈ 150000

daily_branch_performance = 36,500

If counts deviate hugely (e.g., 0 rows), the loader didn’t run or wrote to a different DB/schema.

7) UI wiring & verification

Ensure the front-end / server config is pointed to the intended DB:

For server-side services that write to Supabase, ensure they use SUPABASE_SERVICE_ROLE_KEY.

For client-side only reads, use SUPABASE_ANON_KEY.

For any existing DB access in code, update DATABASE_URL or corresponding config to REPLIT_POSTGRES_URL if using Replit Postgres.

Run UI locally (or in Replit) and exercise these flows:

Customer list page — verify customers load (first page).

Branch dashboard — verify daily performance shows recent rows.

Loan detail page — pick a loan id from SQL and open in UI.

Add a UI verification checklist in README (already planned), including example requests and expected sample ids.

8) If Supabase schema doesn't match Postgres

If the Supabase schema is behind or inconsistent:

Preferred: Overwrite Supabase schema with authoritative SQL file in repo.

Backup Supabase DB first via its SQL export.

Apply supabase_schema.sql from repo.

Alternate: Keep authoritative schema in Postgres and adjust Supabase schema migration SQL to match Postgres (create ALTER TABLE statements).

After schema sync, re-run loader to populate Supabase.

9) Logging & idempotency

Make loader idempotent by:

Using ON CONFLICT DO NOTHING (or upsert logic) for inserts.

Loading in batches and logging progress: INSERTED n rows to <table> and errors to a logfile.

When re-running, prefer --truncate option if the intention is to wipe and re-seed (explicit).

10) Final deliverables the next agent should produce (before handing back)

Successful run logs showing CSVs parsed and inserted into both DBs.

A short audit file scripts/load-report.txt with table row counts in both DBs and any anomalies.

README addition under data/README.md with:

How to run loader (dry-run and real).

Required env vars and where to set them.

Verification SQLs (above).

If fix required, a small PR to scripts/load-data.ts implementing:

Dry-run mode.

Batch inserts.

Clear error handling and reporting.

Confirm UI pages load expected data and add notes for future agents.

Quick debug checklist for the loader error

Check the Replit console logs for the scripts/load-data.ts run. Copy full stack trace.

Confirm Node version >= 16 (some packages require newer Node).

Ensure SUPABASE_SERVICE_ROLE_KEY is present and valid — attempt a simple test script to connect:

import { createClient } from '@supabase/supabase-js';
const supa = createClient(process.env.SUPABASE_URL, process.env.SUPABASE_SERVICE_ROLE_KEY);
(async ()=> console.log(await supa.from('branches').select('*').limit(1)))();


If TypeScript compile error: run npx ts-node --transpile-only scripts/load-data.ts or compile to JS and run.

If DB errors: check constraint violations (missing referenced rows), then load parent tables first.

One-line summary to hand off

“Use the committed CSVs in data/seed/ (do not re-generate), confirm/patch schema parity between Replit Postgres and Supabase, fix & run scripts/load-data.ts in dry-run then real mode using SUPABASE_SERVICE_ROLE_KEY and REPLIT_POSTGRES_URL, verify counts with the provided SQLs, wire env vars for the UI, and produce a load report and README entries for future agents.”